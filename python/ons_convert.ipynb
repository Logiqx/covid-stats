{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONS Convert\n",
    "\n",
    "Created by Michael George (AKA Logiqx)\n",
    "\n",
    "Website: https://logiqx.github.io/covid-stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Standard python libraries plus determination of projdir, basic printable class, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import csv\n",
    "from xlrd import open_workbook, xldate_as_tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import common_core\n",
    "import ons_core\n",
    "import ons_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants Relating to ONS Spreadsheets\n",
    "\n",
    "Text strings to avoid hard-coded values throughout the code; avoids clutter and silent errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly worksheet names (lower case)\n",
    "WEEKLY_FIGURES_LOWER = \"weekly figures 20\"\n",
    "ESTIMATED_TOTAL_DEATHS_LOWER = \"estimated total deaths\"\n",
    "COVID_WEEKLY_REGISTRATIONS_LOWER = \"covid-19 - weekly registrations\"\n",
    "COVID_WEEKLY_OCCURRENCES_LOWER = \"covid-19 - weekly occurrences\"\n",
    "\n",
    "# Daily worksheet names (lower case)\n",
    "REGIONAL_LOWER = \"regional\"\n",
    "\n",
    "# Text used to find specific lines\n",
    "WEEK_NUMBER_TEXT = \"Week number\"\n",
    "WEEK_ENDED_TEXT = \"Week ended\"\n",
    "\n",
    "# Regular expressions used to find specific lines\n",
    "TOTAL_DEATHS_REGEX = \"^Total deaths, all ages\"\n",
    "TOTAL_OCCURRENCES_REGEX = \"^Estimated total death occurrences( \\(2020/2021\\))?$\"\n",
    "COVID_DEATHS_REGEX = \"^Deaths involving COVID-19, all ages\"\n",
    "RESPIRATORY_REGEX = \".*ICD-10 J00-J99.*\"\n",
    "\n",
    "# Maximum number of columns to search for text / regex\n",
    "MAX_COLS_WITH_HEADERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facilitate Spreadsheet Parsing\n",
    "\n",
    "Find specific lines in the spreadsheet, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRowNo(sheet, heading, aliases={}):\n",
    "    '''Find rows with the specified headings. Also check for possible aliases.'''\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Search for row headings with precise wording\n",
    "    headingLower = heading.lower()\n",
    "\n",
    "    # Aliases are still regarded as precise wording\n",
    "    if heading in aliases:\n",
    "        aliasesLower = [alias.lower() for alias in aliases[heading]]\n",
    "    else:\n",
    "        aliasesLower = []\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        for colNo in range(MAX_COLS_WITH_HEADERS):\n",
    "            cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "            if isinstance(cellValue, str):\n",
    "                cellValueLower = cellValue.lower()\n",
    "                if cellValueLower == headingLower or cellValueLower in aliasesLower:\n",
    "                    matches.append(rowNo)\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        rowNo = -1\n",
    "    elif len(matches) > 1:\n",
    "        raise RuntimeError(f\"'{heading}' found in '{sheet.name} multiple times - rows {[match + 1 for match in matches]}\")\n",
    "    else:\n",
    "        rowNo = matches[0]\n",
    "\n",
    "    return rowNo\n",
    "\n",
    "\n",
    "def regexFindRowNos(sheet, pattern, verbose=common_core.verbose):\n",
    "    '''Find rows with the specified headings. Also check for possible aliases.'''\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Pre-compile regex for minor speedup\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        for colNo in range(MAX_COLS_WITH_HEADERS):\n",
    "            cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "            if isinstance(cellValue, str):\n",
    "                if regex.match(cellValue):\n",
    "                    matches.append(rowNo)\n",
    "\n",
    "    if len(matches) > 1 and verbose:\n",
    "        print(f\"WARNING: '{pattern}' found in '{sheet.name}' multiple times - rows {[match + 1 for match in matches]}\")\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        rowNo = -1\n",
    "    else:\n",
    "        rowNo = matches[0]\n",
    "\n",
    "    return rowNo\n",
    "\n",
    "\n",
    "def getWeekColNos(sheet):\n",
    "    '''Determine the columns of week numbers from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_NUMBER_TEXT)\n",
    "    colNos = []\n",
    "\n",
    "    for colNo in range(sheet.ncols):\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        # If the cell contains a value that can be converted to an integer then treat it as a week number\n",
    "        try:\n",
    "            intValue = int(cellValue)\n",
    "            colNos.append(colNo)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return colNos\n",
    "\n",
    "\n",
    "def getWeekNumbers(sheet, colNos):\n",
    "    '''Determine the week numbers from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_NUMBER_TEXT)\n",
    "    weekNumbers = []\n",
    "\n",
    "    for colNo in colNos:\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        weekNumbers.append(int(cellValue))\n",
    "\n",
    "    return weekNumbers\n",
    "\n",
    "\n",
    "def getWeekEndings(sheet, colNos, dateMode):\n",
    "    '''Determine the week endings from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_ENDED_TEXT)\n",
    "    weekEndings = []\n",
    "\n",
    "    for colNo in colNos:\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        if isinstance(cellValue, str):\n",
    "            weekEnding = datetime.strptime(cellValue, '%d-%b-%y').strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            year, month, day, hour, minute, second = xldate_as_tuple(cellValue, dateMode)\n",
    "            weekEnding = f\"{year:04}-{month:02}-{day:02}\"\n",
    "\n",
    "        if datetime.strptime(weekEnding, \"%Y-%m-%d\").weekday() != 4:\n",
    "            print(f\"WARNING: {weekEnding} is not a Friday\")\n",
    "\n",
    "        # Patch typos in worksheet \"Estimated total deaths 2020\"\n",
    "        # Spotted in publishedweek022021.xlsx and publishedweek082021.xlsx\n",
    "        if weekEnding in ['2020-01-01', '2020-02-26']:\n",
    "            print(f\"WARNING: Converting {weekEnding} to {weekEnding.replace('2020', '2021')}\")\n",
    "            weekEnding = weekEnding.replace('2020', '2021')\n",
    "\n",
    "        weekEndings.append(weekEnding)\n",
    "\n",
    "    return weekEndings\n",
    "\n",
    "\n",
    "def getWeekOffsets(weekEndings):\n",
    "    '''Determine the week endings from the cells in the specified row.'''\n",
    "\n",
    "    weekOffsets = []\n",
    "\n",
    "    for weekEnding in weekEndings:\n",
    "        weekEnding = datetime.strptime(weekEnding, \"%Y-%m-%d\")\n",
    "\n",
    "        delta = weekEnding.date() - ons_core.minWeek\n",
    "        weekOffset = delta.days // 7\n",
    "\n",
    "        weekOffsets.append(weekOffset)\n",
    "\n",
    "    return weekOffsets\n",
    "\n",
    "\n",
    "def getCellValue(sheet, rowNo, colNo):\n",
    "    '''Determine the weekly deaths from the cells in the specified row.'''\n",
    "\n",
    "    cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "    # 2011 switched from ICD-10 v 2001 to ICD-10 v 2010 (NCHS)\n",
    "    # 2014 switched from ICD-10 v 2010 (NCHS) to ICD-10 v 2013 (IRIS)\n",
    "    if cellValue == \":\":\n",
    "        cellValue = 0\n",
    "\n",
    "    else:\n",
    "        # Allow non-integers to be treated as zero but show a warning\n",
    "        try:\n",
    "            if cellValue != \"\":\n",
    "                cellValue = int(cellValue)\n",
    "            else:\n",
    "                cellValue = 0\n",
    "        except:\n",
    "            print(f\"Warning: Failed to convert '{cellValue}' to integer in '{sheet.name}' (row {rowNo + 1} col {colNo + 1})\")\n",
    "            cellValue = 0\n",
    "\n",
    "    return cellValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weekly Deaths\n",
    "\n",
    "Load weekly deaths  data into the cache as a means of converting from XLSX to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorkbookYear(workbook):\n",
    "    '''Scan the specified workbook to determine the year.'''\n",
    "\n",
    "    year = None\n",
    "\n",
    "    for sheet in workbook.sheets():\n",
    "        if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "            year = int(sheet.name[-4:])\n",
    "\n",
    "    if year == None:\n",
    "        raise RuntimeError(f\"Year could not be determined for workbook\")\n",
    "\n",
    "    return year\n",
    "\n",
    "\n",
    "def initCache(cache, areaNames, verbose=common_core.verbose):\n",
    "    '''Initialise cache for an individual region'''\n",
    "\n",
    "    dtype = {'names':[ons_core.WEEK_ENDED, ons_core.WEEK_NUMBER,\n",
    "                      ons_core.TOTAL_REGISTRATIONS, ons_core.TOTAL_OCCURRENCES,\n",
    "                      ons_core.COVID_REGISTRATIONS, ons_core.COVID_OCCURRENCES],\n",
    "             'formats':['U10', 'B', 'I', 'I', 'I', 'I']}\n",
    "\n",
    "    # Calculate the maximum array length\n",
    "    delta = ons_core.maxWeek - ons_core.minWeek\n",
    "    maxWeeks = delta.days // 7 + 1\n",
    "\n",
    "    # Allocate cache for the region\n",
    "    for areaName in areaNames:\n",
    "        if areaName not in cache:\n",
    "            if verbose:\n",
    "                print(f\"Initialising {areaName}...\")\n",
    "\n",
    "            cache[areaName] = np.zeros(maxWeeks, dtype=dtype)\n",
    "\n",
    "            # Pre-populate week_ended\n",
    "            for weeksDelta in range(maxWeeks):\n",
    "                weekEnding = ons_core.minWeek + timedelta(weeks=weeksDelta)\n",
    "                cache[areaName][ons_core.WEEK_ENDED][weeksDelta] = weekEnding.strftime(\"%Y-%m-%d\")\n",
    "                cache[areaName][ons_core.WEEK_NUMBER][weeksDelta] = common_core.getOnsWeek(weekEnding)[1]\n",
    "\n",
    "\n",
    "def processAreas(cache, sheetsInfo, areaNames, verbose=common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific region.'''\n",
    "\n",
    "    initCache(cache, areaNames, verbose=verbose)\n",
    "\n",
    "    for sheetInfoKey in sheetsInfo:\n",
    "        sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "\n",
    "        sheet = sheetInfo[\"sheet\"]\n",
    "        weekColNos = sheetInfo[ons_core.WEEK_COL_NOS]\n",
    "        weekNumbers = sheetInfo[ons_core.WEEK_NUMBERS]\n",
    "        weekEndings = sheetInfo[ons_core.WEEK_ENDINGS]\n",
    "        weekOffsets = sheetInfo[ons_core.WEEK_OFFSETS]\n",
    "\n",
    "        for areaName in areaNames:\n",
    "            if areaName in sheetInfo[\"regions\"]:\n",
    "                rowNo = sheetInfo[\"regions\"][areaName]\n",
    "\n",
    "                for i in range(len(weekColNos)):\n",
    "                    cellValue = getCellValue(sheet, rowNo, weekColNos[i])\n",
    "\n",
    "                    weekNumber = weekNumbers[i]\n",
    "                    weekEnding = weekEndings[i]\n",
    "                    weekOffset = weekOffsets[i]\n",
    "\n",
    "                    if (cache[areaName][ons_core.WEEK_ENDED][weekOffset] != weekEnding):\n",
    "                        print(f\"Week mismatch - {cache[areaName][ons_core.WEEK_ENDED][weekOffset]} vs {weekEnding}\")\n",
    "                        raise RuntimeError(f\"Bug in week offset calculations? week {weekNumber} in {sheet.name}\")\n",
    "\n",
    "                    cache[areaName][ons_core.WEEK_NUMBER][weekOffset] = weekNumber\n",
    "                    cache[areaName][sheetInfoKey][weekOffset] = cellValue\n",
    "\n",
    "\n",
    "def processRegions(cache, sheetsInfo, verbose=common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific region.'''\n",
    "\n",
    "    regionNames = []\n",
    "\n",
    "    for regionName in common_core.regionNames:\n",
    "        found = False\n",
    "\n",
    "        for sheetInfoKey in sheetsInfo:\n",
    "            sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "            sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "            rowNo = findRowNo(sheet, regionName, aliases=common_core.regionAliases)\n",
    "\n",
    "            if rowNo >= 0:\n",
    "                sheetInfo[\"regions\"][regionName] = rowNo\n",
    "                found = True\n",
    "\n",
    "        if found:\n",
    "            regionNames.append(regionName)\n",
    "\n",
    "    processAreas(cache, sheetsInfo, regionNames, verbose=verbose)\n",
    "\n",
    "\n",
    "def processNations(cache, sheetsInfo, verbose=common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific country.'''\n",
    "\n",
    "    nationNames = []\n",
    "\n",
    "    for nationName in common_core.nationNames:\n",
    "        found = False\n",
    "\n",
    "        for sheetInfoKey in sheetsInfo:\n",
    "            sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "            sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "            if nationName == common_core.ENGLAND_WALES:\n",
    "                if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "                    rowNo = regexFindRowNos(sheet, TOTAL_DEATHS_REGEX)\n",
    "                elif sheet.name.lower().startswith(ESTIMATED_TOTAL_DEATHS_LOWER):\n",
    "                    rowNo = regexFindRowNos(sheet, TOTAL_OCCURRENCES_REGEX)\n",
    "                else:\n",
    "                    rowNo = regexFindRowNos(sheet, COVID_DEATHS_REGEX)\n",
    "            else:\n",
    "                rowNo = findRowNo(sheet, nationName)\n",
    "\n",
    "            if rowNo >= 0:\n",
    "                sheetInfo[\"regions\"][nationName] = rowNo\n",
    "                found = True\n",
    "\n",
    "        if found:\n",
    "            nationNames.append(nationName)\n",
    "\n",
    "    processAreas(cache, sheetsInfo, nationNames, verbose=verbose)\n",
    "\n",
    "\n",
    "def processWeeklySheets(cache, sheetsInfo, dateMode, verbose=common_core.verbose):\n",
    "    '''Parse the specified worksheets for weekly deaths.'''\n",
    "\n",
    "    for sheetInfoKey in sheetsInfo:\n",
    "        sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "        sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "        weekColNos = getWeekColNos(sheet)\n",
    "        sheetInfo[ons_core.WEEK_COL_NOS] = weekColNos\n",
    "\n",
    "        weekNos = getWeekNumbers(sheet, weekColNos)\n",
    "        assert len(weekNos) == len(weekColNos), f\"Number of week numbers did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_NUMBERS] = weekNos\n",
    "\n",
    "        weekEndings = getWeekEndings(sheet, weekColNos, dateMode)\n",
    "        assert len(weekEndings) == len(weekColNos), f\"Number of week endings did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_ENDINGS] = weekEndings\n",
    "\n",
    "        weekOffsets = getWeekOffsets(weekEndings)\n",
    "        assert len(weekOffsets) == len(weekEndings), f\"Number of week offsets did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_OFFSETS] = weekOffsets\n",
    "\n",
    "        sheetInfo[\"regions\"] = {}\n",
    "\n",
    "    processNations(cache, sheetsInfo, verbose=verbose)\n",
    "    processRegions(cache, sheetsInfo, verbose=verbose)\n",
    "\n",
    "\n",
    "def processWeeklyWorkbook(cache, workbook, verbose=common_core.verbose):\n",
    "    '''Parse the specified workbook for weekly deaths.'''\n",
    "\n",
    "    sheetsInfo = {}\n",
    "\n",
    "    for sheet in workbook.sheets():\n",
    "        if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "            sheetsInfo[ons_core.TOTAL_REGISTRATIONS] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower().startswith(ESTIMATED_TOTAL_DEATHS_LOWER):\n",
    "            sheetsInfo[ons_core.TOTAL_OCCURRENCES] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower() == COVID_WEEKLY_REGISTRATIONS_LOWER:\n",
    "            sheetsInfo[ons_core.COVID_REGISTRATIONS] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower() == COVID_WEEKLY_OCCURRENCES_LOWER:\n",
    "            sheetsInfo[ons_core.COVID_OCCURRENCES] = {\"sheet\": sheet}\n",
    "\n",
    "    processWeeklySheets(cache, sheetsInfo, workbook.datemode, verbose=verbose)\n",
    "\n",
    "\n",
    "def loadWeeklyDeaths(partNames, verbose=common_core.verbose):\n",
    "    '''Load the specified spreadsheets into cache.'''\n",
    "\n",
    "    # Iterate through all workbooks to determine the years\n",
    "    years = {}\n",
    "    for partName in partNames:\n",
    "        fileName = os.path.join(common_core.dataDir, partName)\n",
    "        if verbose:\n",
    "            print(f\"Loading {partName}...\")\n",
    "        workbook = open_workbook(fileName)\n",
    "\n",
    "        year = getWorkbookYear(workbook)\n",
    "        years[year] = workbook\n",
    "\n",
    "    # Iterate throught the years in chronological order - required to handle the 2021 hybrid!\n",
    "    cache = {}\n",
    "    for year in sorted(years):\n",
    "        try:\n",
    "            processWeeklyWorkbook(cache, years[year], verbose=verbose)\n",
    "        except:\n",
    "            print(f\"ERROR: Exception raise whilst processing workbook for {year}\")\n",
    "            raise\n",
    "\n",
    "    return(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Daily Deaths\n",
    "\n",
    "Load daily occurrence data into the cache as a means of converting from XLSX to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDailySheet(cache, sheet, verbose=common_core.verbose):\n",
    "    '''Process daily occurrences in worksheet'''\n",
    "\n",
    "    headerFound = False\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        if headerFound == False:\n",
    "            # Primitive check to determine presence of header\n",
    "            if (sheet.cell(rowNo, 0).value == \"Year\" and\n",
    "                sheet.cell(rowNo, 1).value == \"Month\" and\n",
    "                sheet.cell(rowNo, 2).value == \"Day\" and\n",
    "                sheet.cell(rowNo, 3).value == \"Region\" and\n",
    "                sheet.cell(rowNo, 4).value == \"Deaths\"):\n",
    "                headerFound = True\n",
    "        else:\n",
    "            # Stop at the first empty row\n",
    "            if sheet.cell(rowNo, 0).value == \"\":\n",
    "                break\n",
    "\n",
    "            # Extract data from current row\n",
    "            year, month, day = int(sheet.cell(rowNo, 0).value), int(sheet.cell(rowNo, 1).value), int(sheet.cell(rowNo, 2).value)\n",
    "            areaCode = sheet.cell(rowNo, 3).value\n",
    "            deaths = int(sheet.cell(rowNo, 4).value)\n",
    "\n",
    "            # Handle non-standard nation codes such as W99999999\n",
    "            if areaCode in common_core.nationMappings:\n",
    "                areaCode = common_core.nationMappings[areaCode]\n",
    "\n",
    "            # Initialise cache if necessary\n",
    "            if areaCode not in cache:\n",
    "                cache[areaCode] = []\n",
    "\n",
    "            # Store row in cache\n",
    "            cache[areaCode].append([f\"{year:04}-{month:02}-{day:02}\", deaths])\n",
    "\n",
    "\n",
    "def validateDailyCache(cache, areaCode, verbose=common_core.verbose):\n",
    "    '''Sort the daily occurrences for the specified area and report non-contiguous dates'''\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Validating daily cache for {areaCode}...\")\n",
    "\n",
    "    # Sort data chronologically\n",
    "    cache[areaCode].sort()\n",
    "\n",
    "    # Ensure that dates are contiguous\n",
    "    prevDate = None\n",
    "    for record in cache[areaCode]:\n",
    "        currDate = datetime.strptime(record[0], '%Y-%m-%d')\n",
    "\n",
    "        if prevDate and currDate != prevDate + timedelta(days=1):\n",
    "            raise RuntimeError(f\"Non-contiguous dates for {areaCode} - {record[0]}\")\n",
    "\n",
    "        prevDate = currDate\n",
    "\n",
    "\n",
    "def loadDailyDeaths(verbose=common_core.verbose):\n",
    "    '''Load available daily deaths into cache.'''\n",
    "\n",
    "    rawPath = os.path.join(common_core.dataDir, ons_core.ONS_DEATHS, \"raw\", \"daily\")\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    for baseName in os.listdir(rawPath):\n",
    "        # Ignore temporary files related to spreadhseets that are open\n",
    "        if not baseName.startswith(\"~$\"):\n",
    "            fileName = os.path.join(rawPath, baseName)\n",
    "            if verbose:\n",
    "                print(f\"Loading {baseName}...\")\n",
    "            workbook = open_workbook(fileName)\n",
    "\n",
    "            for sheet in workbook.sheets():\n",
    "                if sheet.name.lower().startswith(REGIONAL_LOWER):\n",
    "                    processDailySheet(cache, sheet, verbose=verbose)\n",
    "\n",
    "    for areaCode in cache:\n",
    "        validateDailyCache(cache, areaCode, verbose=verbose)\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDailyEwmSheet(cache, nationName, sheet, verbose=common_core.verbose):\n",
    "    '''Process daily occurrences in worksheet'''\n",
    "\n",
    "    headerFound = False\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        if headerFound == False:\n",
    "            # Primitive check to determine presence of header\n",
    "            if (sheet.cell(rowNo, 0).value == \"Date\" and\n",
    "                sheet.cell(rowNo, 1).value == \"Number of daily deaths\"):\n",
    "                headerFound = True\n",
    "        else:\n",
    "            # Stop at the first empty row\n",
    "            if sheet.cell(rowNo, 0).value == \"\":\n",
    "                break\n",
    "\n",
    "            # Date format can vary from year to year in the EWM spreadsheets\n",
    "            value = sheet.cell(rowNo, 0).value\n",
    "            try:\n",
    "                if \"/\" in value:\n",
    "                    if len(value) == 10:\n",
    "                        ymd = datetime.strptime(value, '%d/%m/%Y')\n",
    "                    else:\n",
    "                        ymd = datetime.strptime(value, '%d/%m/%y')\n",
    "                elif \" \" in value:\n",
    "                    ymd = datetime.strptime(value, '%d %b %y')\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Unsupported date format {value}\")\n",
    "            except:\n",
    "                print(f\"Error trying to parse {value}\")\n",
    "                raise\n",
    "\n",
    "            deaths = int(sheet.cell(rowNo, 1).value)\n",
    "\n",
    "            # Initialise cache if necessary\n",
    "            if nationName not in cache:\n",
    "                cache[nationName] = []\n",
    "\n",
    "            # Store row in cache\n",
    "            cache[nationName].append([ymd.strftime(\"%Y-%m-%d\"), deaths])\n",
    "\n",
    "\n",
    "def validateDailyEwmCache(cache, nationName, verbose=common_core.verbose):\n",
    "    '''Sort the daily occurrences for the specified area and report non-contiguous dates'''\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Validating daily cache for {nationName}...\")\n",
    "\n",
    "    # Sort data chronologically\n",
    "    cache[nationName].sort()\n",
    "\n",
    "    # Ensure that dates are contiguous\n",
    "    prevDate = None\n",
    "    for record in cache[nationName]:\n",
    "        currDate = datetime.strptime(record[0], '%Y-%m-%d')\n",
    "\n",
    "        if prevDate and currDate != prevDate + timedelta(days=1):\n",
    "            raise RuntimeError(f\"Non-contiguous dates for {nationName} - {record[0]}\")\n",
    "\n",
    "        prevDate = currDate\n",
    "\n",
    "\n",
    "def loadDailyEwmDeaths(verbose=common_core.verbose):\n",
    "    '''Load available daily deaths into cache.'''\n",
    "\n",
    "    rawPath = os.path.join(common_core.dataDir, ons_core.ONS_EWM_DEATHS, \"raw\", \"daily\")\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    for baseName in os.listdir(rawPath):\n",
    "        # Ignore temporary files related to spreadhseets that are open\n",
    "        if not baseName.startswith(\"~$\"):\n",
    "            if common_core.ENGLAND in baseName:\n",
    "                nationName = common_core.ENGLAND\n",
    "            elif common_core.WALES in baseName:\n",
    "                nationName = common_core.WALES\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unrecognised filename {baseName}\")\n",
    "\n",
    "            fileName = os.path.join(rawPath, baseName)\n",
    "            if verbose:\n",
    "                print(f\"Loading {baseName}...\")\n",
    "            workbook = open_workbook(fileName)\n",
    "\n",
    "            for sheet in workbook.sheets():\n",
    "                processDailyEwmSheet(cache, nationName, sheet, verbose=verbose)\n",
    "\n",
    "    for nationName in cache:\n",
    "        validateDailyEwmCache(cache, nationName, verbose=verbose)\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Enhancement\n",
    "\n",
    "Enhance data in the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDailyDeaths(cache, verbose=common_core.verbose):\n",
    "    '''Apply daily deaths from historical extracts'''\n",
    "\n",
    "    dailyDeaths = ons_core.loadCsvFiles(ons_core.ONS_DEATHS, \"daily\", verbose=verbose)\n",
    "\n",
    "    for areaName in dailyDeaths:\n",
    "\n",
    "        # Determine with the first \"week ended\" in the daily cache\n",
    "        weekEnded = dailyDeaths[areaName][0][\"date\"]\n",
    "\n",
    "        # Locate the first suitable week ended in the main cache\n",
    "        cacheIdx = np.where(cache[areaName][ons_core.WEEK_ENDED] >= weekEnded)[0][0]\n",
    "        weekEnded = cache[areaName][cacheIdx][ons_core.WEEK_ENDED]\n",
    "\n",
    "        # Locate this date in the daily occurrences\n",
    "        matches = np.where(dailyDeaths[areaName][\"date\"] == weekEnded)[0]\n",
    "\n",
    "        # Processing can only proceed if the date was found\n",
    "        if len(matches) > 0:\n",
    "            dailyIdx = matches[0]\n",
    "\n",
    "            # Calculate 7 day rolling totals for daily occurrences\n",
    "            # TODO - Consider optimising this and only calculating for limited rows?\n",
    "            rollingSums = common_core.rollingSum(dailyDeaths[areaName][ons_core.TOTAL_OCCURRENCES], window = 7)\n",
    "\n",
    "            # Take a copy of the days which are aligned with \"week ended\"\n",
    "            fridayTotals = rollingSums[dailyIdx::7]\n",
    "            cache[areaName][cacheIdx:cacheIdx + len(fridayTotals)][ons_core.TOTAL_OCCURRENCES] = fridayTotals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEngland(cache, verbose=common_core.verbose):\n",
    "    '''Create data for England (if not already present) using England and Wales'''\n",
    "\n",
    "    # Remove England from the cache if it already exists\n",
    "    cache.pop(common_core.ENGLAND, None)\n",
    "\n",
    "    # Create England from the individual regions\n",
    "    for regionName in common_core.regionNames:\n",
    "        if common_core.ENGLAND in cache:\n",
    "            cache[common_core.ENGLAND][ons_core.TOTAL_REGISTRATIONS] += cache[regionName][ons_core.TOTAL_REGISTRATIONS]\n",
    "            cache[common_core.ENGLAND][ons_core.TOTAL_OCCURRENCES] += cache[regionName][ons_core.TOTAL_OCCURRENCES]\n",
    "\n",
    "            cache[common_core.ENGLAND][ons_core.COVID_REGISTRATIONS] += cache[regionName][ons_core.COVID_REGISTRATIONS]\n",
    "            cache[common_core.ENGLAND][ons_core.COVID_OCCURRENCES] += cache[regionName][ons_core.COVID_OCCURRENCES]\n",
    "\n",
    "        else:\n",
    "            cache[common_core.ENGLAND] = np.copy(cache[regionName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDailyEwmDeaths(cache, verbose=common_core.verbose):\n",
    "    '''Apply daily deaths from Excess Winter Mortality reports'''\n",
    "\n",
    "    dailyDeaths = ons_core.loadCsvFiles(ons_core.ONS_EWM_DEATHS, \"daily\", verbose=verbose)\n",
    "\n",
    "    for nationName in dailyDeaths:\n",
    "\n",
    "        # Determine with the first \"week ended\" in the daily cache\n",
    "        for dailyIdx in range(7):\n",
    "            weekEnded = dailyDeaths[nationName][dailyIdx][\"date\"]\n",
    "            if datetime.strptime(weekEnded, \"%Y-%m-%d\").weekday() == 4:\n",
    "                break\n",
    "\n",
    "        # Locate the week ended in the main cache\n",
    "        cacheIdx = np.where(cache[nationName][ons_core.WEEK_ENDED] == weekEnded)[0][0]\n",
    "\n",
    "        # Locate the first week in main cache without total_occurrences\n",
    "        cacheIdx = np.where(cache[nationName][cacheIdx:][ons_core.TOTAL_OCCURRENCES] == 0)[0][0] + cacheIdx\n",
    "        weekEnded = cache[nationName][cacheIdx][ons_core.WEEK_ENDED]\n",
    "\n",
    "        # Locate this date in the daily occurrences\n",
    "        matches = np.where(dailyDeaths[nationName][\"date\"] == weekEnded)[0]\n",
    "\n",
    "        # Processing can only proceed if the date was found\n",
    "        if len(matches) > 0:\n",
    "            dailyIdx = matches[0]\n",
    "\n",
    "            # Calculate 7 day rolling totals for daily occurrences\n",
    "            # TODO - Consider optimising this and only calculating for limited rows?\n",
    "            rollingSums = common_core.rollingSum(dailyDeaths[nationName][ons_core.TOTAL_OCCURRENCES], window = 7)\n",
    "\n",
    "            # Take a copy of the days which are aligned with \"week ended\"\n",
    "            fridayTotals = rollingSums[dailyIdx::7]\n",
    "            fridayTotals += cache[nationName][cacheIdx:cacheIdx + len(fridayTotals)][ons_core.COVID_OCCURRENCES]\n",
    "            cache[nationName][cacheIdx:cacheIdx + len(fridayTotals)][ons_core.TOTAL_OCCURRENCES] = fridayTotals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchEnglandWales(cache, verbose=common_core.verbose):\n",
    "    '''Patch combined figures for England and Wales using individual figures for England and Wales'''\n",
    "\n",
    "    # Find the first non-zero value in England and Wales\n",
    "    idx = np.where(cache[common_core.ENGLAND_WALES][ons_core.TOTAL_OCCURRENCES] > 0)[0][0]\n",
    "\n",
    "    # Patch everything before that value using individual nations\n",
    "    cache[common_core.ENGLAND_WALES][:idx][ons_core.TOTAL_OCCURRENCES] = \\\n",
    "        cache[common_core.ENGLAND][:idx][ons_core.TOTAL_OCCURRENCES] + \\\n",
    "        cache[common_core.WALES][:idx][ons_core.TOTAL_OCCURRENCES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEstimatedOccurrences(cache, verbose=common_core.verbose):\n",
    "    \"\"\"Calculate missing values in cache\"\"\"\n",
    "\n",
    "    estimates = {}\n",
    "\n",
    "    # Use the ONS occurrences for England and Wales\n",
    "    masterArea = common_core.ENGLAND_WALES\n",
    "\n",
    "    # Non-COVID registrations for the master area, shifted left by ~3.5 days\n",
    "    knownRegistrations = ons_core.shiftRegistrations(cache[masterArea][ons_core.TOTAL_REGISTRATIONS] -\n",
    "                                            cache[masterArea][ons_core.COVID_REGISTRATIONS])\n",
    "\n",
    "    # Non-COVID occurrences for the master area without any shift\n",
    "    knownOccurrences =  cache[masterArea][ons_core.TOTAL_OCCURRENCES] - cache[masterArea][ons_core.COVID_OCCURRENCES]\n",
    "\n",
    "    # Run this process for all regions and nations other than the master area\n",
    "    for areaName in common_core.regionNames + common_core.nationNames:\n",
    "        if areaName != masterArea and areaName in cache:\n",
    "\n",
    "            # Shift registrations left by half a week\n",
    "            shiftedRegistrations = ons_core.shiftRegistrations(cache[areaName][ons_core.TOTAL_REGISTRATIONS] -\n",
    "                                                     cache[areaName][ons_core.COVID_REGISTRATIONS])\n",
    "\n",
    "            # Estimate occurrences using a simple percentage of the known occurrences\n",
    "            estimatedOccurrences = knownOccurrences * np.divide(shiftedRegistrations, knownRegistrations,\n",
    "                                      out=np.zeros_like(shiftedRegistrations), where=knownRegistrations != 0) + \\\n",
    "                                    cache[areaName][ons_core.COVID_OCCURRENCES]\n",
    "\n",
    "            # Locate the last week in the cache where total_occurrences is populated\n",
    "            cacheIdx = np.where(cache[areaName][ons_core.TOTAL_OCCURRENCES] > 0)[0][-1]\n",
    "\n",
    "            # Patch subsequent data using the estimates\n",
    "            cache[areaName][cacheIdx + 1:][ons_core.TOTAL_OCCURRENCES] = estimatedOccurrences[cacheIdx + 1:]\n",
    "\n",
    "            # Maintain a cache of estimates for testing purpopses\n",
    "            estimates[areaName] = estimatedOccurrences\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateErrors(cache, estimates, verbose=common_core.verbose):\n",
    "    \"\"\"Calculate estimation errors\"\"\"\n",
    "\n",
    "    masterArea = common_core.ENGLAND_WALES\n",
    "\n",
    "    for areaName in common_core.regionNames + common_core.nationNames:\n",
    "        if areaName in cache and areaName != masterArea:\n",
    "            print(f\"{areaName}:\")\n",
    "\n",
    "            totalPctMAE = 0\n",
    "\n",
    "            years = range(2010, 2018)\n",
    "\n",
    "            for year in years:\n",
    "                startDate = f\"{year}-01-01\"\n",
    "                stopDate = f\"{year}-12-31\"\n",
    "\n",
    "                areaData = cache[areaName]\n",
    "\n",
    "                startIdx = np.where(areaData[ons_core.WEEK_ENDED] >= startDate)[0][0]\n",
    "                stopIdx = np.where(areaData[ons_core.WEEK_ENDED] < stopDate)[0][-1]\n",
    "\n",
    "                pctMAE = 100 * np.average(np.abs(areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES] -\n",
    "                                            areaData[startIdx:stopIdx][ons_core.TOTAL_REGISTRATIONS].astype(np.float64)) /\n",
    "                                        areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES])\n",
    "\n",
    "                pctMAE = 100 * np.average(np.abs(areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES] -\n",
    "                                            ons_core.shiftRegistrations(areaData[startIdx:stopIdx][ons_core.TOTAL_REGISTRATIONS])) /\n",
    "                                        areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES])\n",
    "\n",
    "                pctMAE = 100 * np.average(np.abs(areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES] -\n",
    "                                            estimates[areaName][startIdx:stopIdx]) /\n",
    "                                        areaData[startIdx:stopIdx][ons_core.TOTAL_OCCURRENCES])\n",
    "\n",
    "                totalPctMAE += pctMAE\n",
    "\n",
    "                print(f\"{year} = {pctMAE:.2f}%\")\n",
    "\n",
    "            print(f\"Avg  = {totalPctMAE / len(years):.2f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfillCache(cache, estimationReport=False, verbose=common_core.verbose):\n",
    "    '''Fill in missing cache values using additional data sources and derivations'''\n",
    "\n",
    "    # Reset total_occurrences to zero so that they can be re-calculated\n",
    "    for regionName in cache:\n",
    "        if regionName != common_core.ENGLAND_WALES:\n",
    "            cache[regionName][ons_core.TOTAL_OCCURRENCES] = 0\n",
    "\n",
    "    # Apply daily deaths from historical extracts\n",
    "    applyDailyDeaths(cache, verbose=verbose)\n",
    "\n",
    "    # Create England from the regions\n",
    "    createEngland(cache, verbose=verbose)\n",
    "\n",
    "    # Apply daily deaths from Excess Winter Mortality reports\n",
    "    applyDailyEwmDeaths(cache, verbose=verbose)\n",
    "\n",
    "    # Create England and Wales\n",
    "    patchEnglandWales(cache, verbose=verbose)\n",
    "\n",
    "    # Estimate the number of occurrences where necessary\n",
    "    estimates = getEstimatedOccurrences(cache, verbose=verbose)\n",
    "\n",
    "    # Evaluation of estimation errors\n",
    "    if estimationReport:\n",
    "        calculateErrors(cache, estimates, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimCache(cache, verbose=common_core.verbose):\n",
    "    '''Remove rows from cache which don't contain any useful data'''\n",
    "\n",
    "    for areaName in cache:\n",
    "        # Locate the first row to retain\n",
    "        minIdx = np.where(cache[areaName][ons_core.WEEK_ENDED] == '2000-01-07')[0][0]\n",
    "\n",
    "        # Locate the first row to retain - total_registrations or total_occurrences > 0\n",
    "        maxRegIdx = np.where(cache[areaName][ons_core.TOTAL_REGISTRATIONS] > 0)[0][-1]\n",
    "        maxOccIdx = np.where(cache[areaName][ons_core.TOTAL_OCCURRENCES] > 0)[0][-1]\n",
    "\n",
    "        # Trim the area\n",
    "        cache[areaName] = cache[areaName][minIdx:max(maxRegIdx, maxOccIdx) + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cache Data\n",
    "\n",
    "Save data in numpy arrays to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDailyArea(cache, areaType, areaName, areaCode, verbose=common_core.verbose):\n",
    "    '''Save data in cache to CSV'''\n",
    "\n",
    "    if areaCode in cache:\n",
    "        if verbose:\n",
    "            print(f\"Saving {areaName}...\")\n",
    "\n",
    "        # Ensure CSV path exists\n",
    "        csvPath = os.path.join(common_core.dataDir, ons_core.ONS_DEATHS, \"csv\", \"daily\", areaType)\n",
    "        if not os.path.exists(csvPath):\n",
    "            os.makedirs(csvPath)\n",
    "\n",
    "        # Determine safe filename\n",
    "        csvFn = os.path.join(csvPath, common_core.getSafeName(areaName) + \".csv\")\n",
    "\n",
    "        # Save data to CSV\n",
    "        with open(csvFn, 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "\n",
    "            colNames = [\"date\", ons_core.TOTAL_OCCURRENCES]\n",
    "            writer.writerow(colNames)\n",
    "\n",
    "            writer.writerows(cache[areaCode])\n",
    "\n",
    "\n",
    "def saveDailyDeaths(cache, verbose=common_core.verbose):\n",
    "    '''Save all extracted data to CSV'''\n",
    "\n",
    "    for nationCode in common_core.nations:\n",
    "        nationName = common_core.nations[nationCode]\n",
    "        saveDailyArea(cache, \"nation\", nationName, nationCode, verbose=verbose)\n",
    "\n",
    "    for regionCode in common_core.regions:\n",
    "        regionName = common_core.regions[regionCode]\n",
    "        saveDailyArea(cache, \"region\", regionName, regionCode, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDailyEwmNation(cache, nationName, verbose=common_core.verbose):\n",
    "    '''Save data in cache to CSV'''\n",
    "\n",
    "    if nationName in cache:\n",
    "        if verbose:\n",
    "            print(f\"Saving {nationName}...\")\n",
    "\n",
    "        # Ensure CSV path exists\n",
    "        csvPath = os.path.join(common_core.dataDir, ons_core.ONS_EWM_DEATHS, \"csv\", \"daily\", \"nation\")\n",
    "        if not os.path.exists(csvPath):\n",
    "            os.makedirs(csvPath)\n",
    "\n",
    "        # Determine safe filename\n",
    "        csvFn = os.path.join(csvPath, common_core.getSafeName(nationName) + \".csv\")\n",
    "\n",
    "        # Save data to CSV\n",
    "        with open(csvFn, 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "\n",
    "            colNames = [\"date\", ons_core.TOTAL_OCCURRENCES]\n",
    "            writer.writerow(colNames)\n",
    "\n",
    "            writer.writerows(cache[nationName])\n",
    "\n",
    "\n",
    "def saveDailyEwmDeaths(cache, verbose=common_core.verbose):\n",
    "    '''Save all extracted data to CSV'''\n",
    "\n",
    "    for nationName in common_core.nationNames:\n",
    "        saveDailyEwmNation(cache, nationName, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveWeeklyArea(cache, areaType, areaName, verbose=common_core.verbose):\n",
    "    '''Save data in cache to CSV'''\n",
    "\n",
    "    if areaName in cache:\n",
    "        header = ','.join(cache[areaName].dtype.names)\n",
    "\n",
    "        # Ensure CSV path exists\n",
    "        csvPath = os.path.join(common_core.dataDir, ons_core.ONS_DEATHS, \"csv\", \"weekly\", areaType)\n",
    "        if not os.path.exists(csvPath):\n",
    "            os.makedirs(csvPath)\n",
    "\n",
    "        # Determine safe filename\n",
    "        csvFn = os.path.join(csvPath, common_core.getSafeName(areaName) + \".csv\")\n",
    "\n",
    "        if verbose:\n",
    "            partName = common_core.getPartName(csvFn)\n",
    "            print(f\"Saving {partName}...\")\n",
    "\n",
    "        # Save data to CSV\n",
    "        np.savetxt(csvFn, cache[areaName], fmt='%s', delimiter=',', header=header, comments='')\n",
    "\n",
    "\n",
    "def saveWeeklyDeaths(cache, verbose=common_core.verbose):\n",
    "    '''Save all extracted data to CSV'''\n",
    "\n",
    "    for nationName in common_core.nationNames:\n",
    "        saveWeeklyArea(cache, \"nation\", nationName, verbose=verbose)\n",
    "\n",
    "    for regionName in common_core.regionNames:\n",
    "        saveWeeklyArea(cache, \"region\", regionName, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download of ons-deaths/raw/weekly/publishedweek092021.xlsx...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek532020.xlsx...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek522019.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek522018withupdatedrespiratoryrow.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek522017.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek522016.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2015.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2014.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2013.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2012.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2011.xls...\n",
      "Skipping download of ons-deaths/raw/weekly/publishedweek2010.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek092021.xlsx...\n",
      "Loading ons-deaths/raw/weekly/publishedweek532020.xlsx...\n",
      "Loading ons-deaths/raw/weekly/publishedweek522019.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek522018withupdatedrespiratoryrow.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek522017.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek522016.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2015.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2014.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2013.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2012.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2011.xls...\n",
      "Loading ons-deaths/raw/weekly/publishedweek2010.xls...\n",
      "Initialising England and Wales...\n",
      "Initialising Wales...\n",
      "Initialising North East...\n",
      "Initialising North West...\n",
      "Initialising Yorkshire and The Humber...\n",
      "Initialising East Midlands...\n",
      "Initialising West Midlands...\n",
      "Initialising East of England...\n",
      "Initialising London...\n",
      "Initialising South East...\n",
      "Initialising South West...\n",
      "Loading ons-deaths/csv/daily/nation/wales.csv...\n",
      "Loading ons-deaths/csv/daily/region/north_east.csv...\n",
      "Loading ons-deaths/csv/daily/region/north_west.csv...\n",
      "Loading ons-deaths/csv/daily/region/yorkshire_humber.csv...\n",
      "Loading ons-deaths/csv/daily/region/east_midlands.csv...\n",
      "Loading ons-deaths/csv/daily/region/west_midlands.csv...\n",
      "Loading ons-deaths/csv/daily/region/east_england.csv...\n",
      "Loading ons-deaths/csv/daily/region/london.csv...\n",
      "Loading ons-deaths/csv/daily/region/south_east.csv...\n",
      "Loading ons-deaths/csv/daily/region/south_west.csv...\n",
      "Loading ons-ewm-deaths/csv/daily/nation/england.csv...\n",
      "Loading ons-ewm-deaths/csv/daily/nation/wales.csv...\n",
      "Saving ons-deaths/csv/weekly/nation/england_wales.csv...\n",
      "Saving ons-deaths/csv/weekly/nation/england.csv...\n",
      "Saving ons-deaths/csv/weekly/nation/wales.csv...\n",
      "Saving ons-deaths/csv/weekly/region/north_east.csv...\n",
      "Saving ons-deaths/csv/weekly/region/north_west.csv...\n",
      "Saving ons-deaths/csv/weekly/region/yorkshire_humber.csv...\n",
      "Saving ons-deaths/csv/weekly/region/east_midlands.csv...\n",
      "Saving ons-deaths/csv/weekly/region/west_midlands.csv...\n",
      "Saving ons-deaths/csv/weekly/region/east_england.csv...\n",
      "Saving ons-deaths/csv/weekly/region/london.csv...\n",
      "Saving ons-deaths/csv/weekly/region/south_east.csv...\n",
      "Saving ons-deaths/csv/weekly/region/south_west.csv...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    # One-off conversion of daily occurrences - quite slow so don't run repeatedly!\n",
    "    #cache = loadDailyDeaths(verbose=verbose)\n",
    "    #saveDailyDeaths(cache, verbose=verbose)\n",
    "\n",
    "    # One-off conversion of daily occurrences - quite slow so don't run repeatedly!\n",
    "    #cache = loadDailyEwmDeaths(verbose=verbose)\n",
    "    #saveDailyEwmDeaths(cache, verbose=verbose)\n",
    "\n",
    "    # Check website for latest spreadsheets then load into cache\n",
    "    partNames = ons_download.downloadDeaths(verbose=verbose)\n",
    "    cache = loadWeeklyDeaths(partNames, verbose=verbose)\n",
    "\n",
    "    # Alternatively we can use the existing CSV files\n",
    "    #cache = ons_core.loadCsvFiles(ons_core.ONS_DEATHS, \"weekly\", verbose=verbose)\n",
    "\n",
    "    # Tidy up the data, prior to saving as CSV files\n",
    "    polyfillCache(cache, estimationReport=False, verbose=verbose)\n",
    "    trimCache(cache, verbose=verbose)\n",
    "\n",
    "    # Save cache as CSV files\n",
    "    saveWeeklyDeaths(cache, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
