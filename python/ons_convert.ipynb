{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONS Convert\n",
    "\n",
    "Created by Michael George (AKA Logiqx)\n",
    "\n",
    "Website: https://logiqx.github.io/covid-stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Standard python libraries plus determination of projdir, basic printable class, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import csv\n",
    "from xlrd import open_workbook, xldate_as_tuple\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "import common_core\n",
    "import ons_core\n",
    "import ons_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Text strings to avoid hard-coded values throughout the code; avoids clutter and silent errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worksheet names (lower case)\n",
    "WEEKLY_FIGURES_LOWER = \"weekly figures 20\"\n",
    "ESTIMATED_TOTAL_DEATHS_LOWER = \"estimated total deaths\"\n",
    "COVID_WEEKLY_REGISTRATIONS_LOWER = \"covid-19 - weekly registrations\"\n",
    "COVID_WEEKLY_OCCURRENCES_LOWER = \"covid-19 - weekly occurrences\"\n",
    "\n",
    "# Text used to find specific lines\n",
    "WEEK_NUMBER_TEXT = \"Week number\"\n",
    "WEEK_ENDED_TEXT = \"Week ended\"\n",
    "\n",
    "# Regular expressions used to find specific lines\n",
    "TOTAL_DEATHS_REGEX = \"^Total deaths, all ages\"\n",
    "TOTAL_OCCURRENCES_REGEX = \"^Estimated total death occurrences$\"\n",
    "COVID_DEATHS_REGEX = \"^Deaths involving COVID-19, all ages\"\n",
    "RESPIRATORY_REGEX = \".*ICD-10 J00-J99.*\"\n",
    "\n",
    "# Maximum number of columns to search for text / regex\n",
    "MAX_COLS_WITH_HEADERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facilitate Parsing\n",
    "\n",
    "Find specific lines in the spreadsheet, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRowNo(sheet, heading, aliases = {}):\n",
    "    '''Find rows with the specified headings. Also check for possible aliases.'''\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Search for row headings with precise wording\n",
    "    headingLower = heading.lower()\n",
    "\n",
    "    # Aliases are still regarded as precise wording\n",
    "    if heading in aliases:\n",
    "        aliasesLower = [alias.lower() for alias in aliases[heading]]\n",
    "    else:\n",
    "        aliasesLower = []\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        for colNo in range(MAX_COLS_WITH_HEADERS):\n",
    "            cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "            if isinstance(cellValue, str):\n",
    "                cellValueLower = cellValue.lower()\n",
    "                if cellValueLower == headingLower or cellValueLower in aliasesLower:\n",
    "                    matches.append(rowNo)\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        rowNo = -1\n",
    "    elif len(matches) > 1:\n",
    "        raise RuntimeError(f\"'{heading}' found in '{sheet.name} multiple times - rows {[match + 1 for match in matches]}\")\n",
    "    else:\n",
    "        rowNo = matches[0]\n",
    "\n",
    "    return rowNo\n",
    "\n",
    "\n",
    "def regexFindRowNos(sheet, pattern, verbose = common_core.verbose):\n",
    "    '''Find rows with the specified headings. Also check for possible aliases.'''\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Pre-compile regex for minor speedup\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    for rowNo in range(sheet.nrows):\n",
    "        for colNo in range(MAX_COLS_WITH_HEADERS):\n",
    "            cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "            if isinstance(cellValue, str):\n",
    "                if regex.match(cellValue):\n",
    "                    matches.append(rowNo)\n",
    "\n",
    "    if len(matches) > 1 and verbose:\n",
    "        print(f\"WARNING: '{pattern}' found in '{sheet.name}' multiple times - rows {[match + 1 for match in matches]}\")\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        rowNo = -1\n",
    "    else:\n",
    "        rowNo = matches[0]\n",
    "\n",
    "    return rowNo\n",
    "\n",
    "\n",
    "def getWeekColNos(sheet):\n",
    "    '''Determine the columns of week numbers from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_NUMBER_TEXT)\n",
    "    colNos = []\n",
    "\n",
    "    for colNo in range(sheet.ncols):\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        # If the cell contains a value that can be converted to an integer then treat it as a week number\n",
    "        try:\n",
    "            intValue = int(cellValue)\n",
    "            colNos.append(colNo)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return colNos\n",
    "\n",
    "\n",
    "def getWeekNumbers(sheet, colNos):\n",
    "    '''Determine the week numbers from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_NUMBER_TEXT)\n",
    "    weekNumbers = []\n",
    "\n",
    "    for colNo in colNos:\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        weekNumbers.append(int(cellValue))\n",
    "\n",
    "    return weekNumbers\n",
    "\n",
    "\n",
    "def getWeekEndings(sheet, colNos, dateMode):\n",
    "    '''Determine the week endings from the cells in the specified row.'''\n",
    "\n",
    "    rowNo = findRowNo(sheet, WEEK_ENDED_TEXT)\n",
    "    weekEndings = []\n",
    "\n",
    "    for colNo in colNos:\n",
    "        cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "        if isinstance(cellValue, str):\n",
    "            weekEnding = datetime.strptime(cellValue, '%d-%b-%y')\n",
    "            weekEndings.append(weekEnding.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            year, month, day, hour, minute, second = xldate_as_tuple(cellValue, dateMode)\n",
    "            weekEndings.append(f\"{year:04}-{month:02}-{day:02}\")\n",
    "\n",
    "    return weekEndings\n",
    "\n",
    "\n",
    "def getWeekOffsets(weekEndings):\n",
    "    '''Determine the week endings from the cells in the specified row.'''\n",
    "\n",
    "    weekOffsets = []\n",
    "\n",
    "    for weekEnding in weekEndings:\n",
    "        weekEnding = datetime.strptime(weekEnding, \"%Y-%m-%d\")\n",
    "            \n",
    "        delta = weekEnding.date() - ons_core.minWeek\n",
    "        weekOffset = delta.days // 7\n",
    "\n",
    "        weekOffsets.append(weekOffset)\n",
    "\n",
    "    return weekOffsets\n",
    "\n",
    "\n",
    "def getCellValue(sheet, rowNo, colNo):\n",
    "    '''Determine the weekly deaths from the cells in the specified row.'''\n",
    "    \n",
    "    cellValue = sheet.cell(rowNo, colNo).value\n",
    "\n",
    "    # 2011 switched from ICD-10 v 2001 to ICD-10 v 2010 (NCHS)\n",
    "    # 2014 switched from ICD-10 v 2010 (NCHS) to ICD-10 v 2013 (IRIS)\n",
    "    if cellValue == \":\":\n",
    "        cellValue = 0\n",
    "\n",
    "    else:\n",
    "        # Allow non-integers to be treated as zero but show a warning\n",
    "        try:\n",
    "            if cellValue != \"\":\n",
    "                cellValue = int(cellValue)\n",
    "            else:\n",
    "                cellValue = 0\n",
    "        except:\n",
    "            print(f\"Warning: Failed to convert '{cellValue}' to integer in '{sheet.name}' (row {rowNo + 1} col {colNo + 1})\")\n",
    "            cellValue = 0\n",
    "\n",
    "    return cellValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Spreadsheet\n",
    "\n",
    "Stuff more specific to the ONS spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initCache(cache, areaNames, verbose = common_core.verbose):\n",
    "    '''Initialise cache for an individual region'''\n",
    "    \n",
    "    dtype = {'names':[ons_core.WEEK_ENDED, ons_core.WEEK_NUMBER,\n",
    "                      ons_core.TOTAL_REGISTRATIONS, ons_core.TOTAL_OCCURRENCES,\n",
    "                      ons_core.COVID_REGISTRATIONS, ons_core.COVID_OCCURRENCES],\n",
    "             'formats':['U10', 'B', 'I', 'I', 'I', 'I']}\n",
    "\n",
    "    # Calculate the maximum array length\n",
    "    delta = ons_core.maxWeek - ons_core.minWeek\n",
    "    maxWeeks = delta.days // 7 + 1\n",
    "\n",
    "    # Allocate cache for the region\n",
    "    for areaName in areaNames:\n",
    "        if areaName not in cache:\n",
    "            if verbose:\n",
    "                print(f\"Initialising {areaName}...\")\n",
    "\n",
    "            cache[areaName] = np.zeros(maxWeeks, dtype=dtype)\n",
    "\n",
    "            # Pre-populate week_ended\n",
    "            for weeksDelta in range(maxWeeks):\n",
    "                weekEnding = ons_core.minWeek + timedelta(weeks=weeksDelta)\n",
    "                cache[areaName][ons_core.WEEK_ENDED][weeksDelta] = weekEnding.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def processAreas(cache, sheetsInfo, areaNames, verbose = common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific region.'''\n",
    "    \n",
    "    initCache(cache, areaNames, verbose = verbose)\n",
    "\n",
    "    for sheetInfoKey in sheetsInfo:\n",
    "        sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "\n",
    "        sheet = sheetInfo[\"sheet\"]\n",
    "        weekColNos = sheetInfo[ons_core.WEEK_COL_NOS]\n",
    "        weekNumbers = sheetInfo[ons_core.WEEK_NUMBERS]\n",
    "        weekEndings = sheetInfo[ons_core.WEEK_ENDINGS]\n",
    "        weekOffsets = sheetInfo[ons_core.WEEK_OFFSETS]\n",
    "\n",
    "        for areaName in areaNames:\n",
    "            if areaName in sheetInfo[\"regions\"]:\n",
    "                rowNo = sheetInfo[\"regions\"][areaName]\n",
    "\n",
    "                for i in range(len(weekColNos)):\n",
    "                    cellValue = getCellValue(sheet, rowNo, weekColNos[i])\n",
    "\n",
    "                    weekNumber = weekNumbers[i]\n",
    "                    weekEnding = weekEndings[i]\n",
    "                    weekOffset = weekOffsets[i]\n",
    "\n",
    "                    assert cache[areaName][ons_core.WEEK_ENDED][weekOffset] == weekEnding, \"Bug in week offset calculations!\"\n",
    "\n",
    "                    cache[areaName][ons_core.WEEK_NUMBER][weekOffset] = weekNumber\n",
    "                    cache[areaName][sheetInfoKey][weekOffset] = cellValue\n",
    "\n",
    "\n",
    "def processRegions(cache, sheetsInfo, verbose = common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific region.'''\n",
    "\n",
    "    regionNames = []\n",
    "    \n",
    "    for regionName in common_core.regionNames:\n",
    "        found = False\n",
    "\n",
    "        for sheetInfoKey in sheetsInfo:\n",
    "            sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "            sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "            rowNo = findRowNo(sheet, regionName, common_core.regionAliases)\n",
    "\n",
    "            if rowNo >= 0:\n",
    "                sheetInfo[\"regions\"][regionName] = rowNo\n",
    "                found = True\n",
    "                \n",
    "        if found:\n",
    "            regionNames.append(regionName)\n",
    "\n",
    "    processAreas(cache, sheetsInfo, regionNames, verbose = verbose)\n",
    "\n",
    "\n",
    "def processNations(cache, sheetsInfo, verbose = common_core.verbose):\n",
    "    '''Parse the specified worksheet for weekly deaths in a specific country.'''\n",
    "\n",
    "    nationNames = []\n",
    "    \n",
    "    for nationName in common_core.nationNames:\n",
    "        found = False\n",
    "\n",
    "        for sheetInfoKey in sheetsInfo:\n",
    "            sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "            sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "            if nationName == common_core.ENGLAND_WALES:\n",
    "                if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "                    rowNo = regexFindRowNos(sheet, TOTAL_DEATHS_REGEX)\n",
    "                elif sheet.name.lower().startswith(ESTIMATED_TOTAL_DEATHS_LOWER):\n",
    "                    rowNo = regexFindRowNos(sheet, TOTAL_OCCURRENCES_REGEX)\n",
    "                else:\n",
    "                    rowNo = regexFindRowNos(sheet, COVID_DEATHS_REGEX)\n",
    "            else:\n",
    "                rowNo = findRowNo(sheet, nationName)\n",
    "\n",
    "            if rowNo >= 0:\n",
    "                sheetInfo[\"regions\"][nationName] = rowNo               \n",
    "                found = True\n",
    "                \n",
    "        if found:\n",
    "            nationNames.append(nationName)\n",
    "                \n",
    "    processAreas(cache, sheetsInfo, nationNames, verbose = verbose)\n",
    "\n",
    "\n",
    "def processSheets(cache, sheetsInfo, dateMode, verbose = common_core.verbose):\n",
    "    '''Parse the specified worksheets for weekly deaths.'''\n",
    "\n",
    "    for sheetInfoKey in sheetsInfo:\n",
    "        sheetInfo = sheetsInfo[sheetInfoKey]\n",
    "        sheet = sheetInfo[\"sheet\"]\n",
    "\n",
    "        weekColNos = getWeekColNos(sheet)\n",
    "        sheetInfo[ons_core.WEEK_COL_NOS] = weekColNos\n",
    "\n",
    "        weekNos = getWeekNumbers(sheet, weekColNos)\n",
    "        assert len(weekNos) == len(weekColNos), f\"Number of week numbers did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_NUMBERS] = weekNos\n",
    "        \n",
    "        weekEndings = getWeekEndings(sheet, weekColNos, dateMode)\n",
    "        assert len(weekEndings) == len(weekColNos), f\"Number of week endings did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_ENDINGS] = weekEndings\n",
    "\n",
    "        weekOffsets = getWeekOffsets(weekEndings)\n",
    "        assert len(weekOffsets) == len(weekEndings), f\"Number of week offsets did not match number of weeks in '{sheet.name}'\"\n",
    "        sheetInfo[ons_core.WEEK_OFFSETS] = weekOffsets\n",
    "\n",
    "        sheetInfo[\"regions\"] = {}\n",
    "\n",
    "    processNations(cache, sheetsInfo, verbose = verbose)\n",
    "    processRegions(cache, sheetsInfo, verbose = verbose)\n",
    "\n",
    "\n",
    "def processWorkbook(cache, workbook, verbose = common_core.verbose):\n",
    "    '''Parse the specified workbook for weekly deaths.'''\n",
    "\n",
    "    sheetsInfo = {}\n",
    "\n",
    "    for sheet in workbook.sheets():           \n",
    "        if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "            sheetsInfo[ons_core.TOTAL_REGISTRATIONS] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower().startswith(ESTIMATED_TOTAL_DEATHS_LOWER):\n",
    "            sheetsInfo[ons_core.TOTAL_OCCURRENCES] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower() == COVID_WEEKLY_REGISTRATIONS_LOWER:\n",
    "            sheetsInfo[ons_core.COVID_REGISTRATIONS] = {\"sheet\": sheet}\n",
    "        elif sheet.name.lower() == COVID_WEEKLY_OCCURRENCES_LOWER:\n",
    "            sheetsInfo[ons_core.COVID_OCCURRENCES] = {\"sheet\": sheet}\n",
    "            \n",
    "    processSheets(cache, sheetsInfo, workbook.datemode, verbose = verbose)\n",
    "\n",
    "\n",
    "def getWorkbookYear(workbook):\n",
    "    '''Scan the specified workbook to determine the year.'''\n",
    "\n",
    "    year = None\n",
    "\n",
    "    for sheet in workbook.sheets():           \n",
    "        if sheet.name.lower().startswith(WEEKLY_FIGURES_LOWER):\n",
    "            year = int(sheet.name[-4:])\n",
    "\n",
    "    if year == None:\n",
    "        raise RuntimeError(f\"Year could not be determined for workbook\")\n",
    "\n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spreadsheet Interface\n",
    "\n",
    "Main interface for converting from XLSX files to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadExcelFiles(partNames, verbose = common_core.verbose):\n",
    "    '''Load the specified spreadsheets into cache.'''\n",
    "\n",
    "    # Iterate through all workbooks to determine the years\n",
    "    years = {}\n",
    "    for partName in partNames:\n",
    "        fileName = os.path.join(common_core.projdir, \"data\", \"ons-deaths\", \"raw\", partName)\n",
    "        workbook = open_workbook(fileName)\n",
    "\n",
    "        year = getWorkbookYear(workbook)\n",
    "        years[year] = workbook\n",
    "\n",
    "    # Iterate throught the years in chronological order - required to handle the 2021 hybrid!\n",
    "    cache = {}\n",
    "    for year in sorted(years):\n",
    "        try:\n",
    "            processWorkbook(cache, years[year], verbose = verbose)\n",
    "        except:\n",
    "            print(f\"ERROR: Exception raise whilst processing workbook for {year}\")\n",
    "            raise\n",
    "\n",
    "    return(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Enhancement\n",
    "\n",
    "Enhance data in the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEngland(cache, verbose = common_core.verbose):\n",
    "    '''Create data for England using England and Wales'''\n",
    "\n",
    "    initCache(cache, [common_core.ENGLAND], verbose = verbose)\n",
    "    \n",
    "    for regionName in common_core.regionNames:\n",
    "        cache[common_core.ENGLAND][ons_core.TOTAL_REGISTRATIONS] += cache[regionName][ons_core.TOTAL_REGISTRATIONS]\n",
    "        cache[common_core.ENGLAND][ons_core.TOTAL_OCCURRENCES] += cache[regionName][ons_core.TOTAL_OCCURRENCES]\n",
    "\n",
    "        cache[common_core.ENGLAND][ons_core.COVID_REGISTRATIONS] += cache[regionName][ons_core.COVID_REGISTRATIONS]\n",
    "        cache[common_core.ENGLAND][ons_core.COVID_OCCURRENCES] += cache[regionName][ons_core.COVID_OCCURRENCES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimCache(cache, verbose = common_core.verbose):\n",
    "    '''Remove rows from cache which don't contain any useful data'''\n",
    "\n",
    "    for areaName in cache:\n",
    "        populated = np.where(cache[areaName][ons_core.TOTAL_REGISTRATIONS] > 0)[0]\n",
    "        cache[areaName] = cache[areaName][populated[0]:populated[-1] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateOccurrencesW53(cache, verbose = common_core.verbose):\n",
    "    '''Calculate the estimated number of occurrences for week 53 of 2020'''\n",
    "\n",
    "    indexW53 = np.where(cache[common_core.ENGLAND_WALES][ons_core.WEEK_ENDED] == '2021-01-01')[0][0]\n",
    "    \n",
    "    # Take minimum of 4 weeks before and 4 weeks afterwards\n",
    "    yBefore = cache[common_core.ENGLAND_WALES][ons_core.TOTAL_OCCURRENCES][indexW53 - 4:indexW53]\n",
    "    yAfter = cache[common_core.ENGLAND_WALES][ons_core.TOTAL_OCCURRENCES][indexW53 + 1:indexW53 + 5]\n",
    "    y = np.hstack((yBefore, yAfter))\n",
    "    \n",
    "    # y values need to correspond to the x values\n",
    "    xBefore = np.arange(len(yBefore))\n",
    "    xAfter = np.arange(len(yAfter)) + 5\n",
    "    x = np.hstack((xBefore, xAfter))\n",
    "    \n",
    "    # Calculate the missing point using Cubic Spline\n",
    "    cs = CubicSpline(x, y)\n",
    "\n",
    "    cache[common_core.ENGLAND_WALES][ons_core.TOTAL_OCCURRENCES][indexW53] = cs(4)\n",
    "    \n",
    "\n",
    "def polyfillCache(cache, verbose = common_core.verbose):\n",
    "    '''Calculate missing values using whatever method is appropriate'''\n",
    "\n",
    "    calculateOccurrencesW53(cache, verbose = verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cache Data\n",
    "\n",
    "Save data in numpy arrays to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveArea(cache, areaType, areaName, verbose = common_core.verbose):\n",
    "    '''Save data in cache to CSV'''\n",
    "\n",
    "    if areaName in cache:\n",
    "        if verbose:\n",
    "            print(f\"Saving {areaName}...\")\n",
    "\n",
    "        header = ','.join(cache[areaName].dtype.names)\n",
    "\n",
    "        # Ensure CSV path exists\n",
    "        csvPath = os.path.join(common_core.projdir, \"data\", \"ons-deaths\", \"csv\", \"weekly\", \"deaths\", areaType)\n",
    "        if not os.path.exists(csvPath):\n",
    "            os.makedirs(csvPath)\n",
    "\n",
    "        # Determine safe filename\n",
    "        csvFn = os.path.join(csvPath, common_core.getSafeName(areaName) + \".csv\")\n",
    "\n",
    "        # Save data to CSV\n",
    "        np.savetxt(csvFn, cache[areaName], fmt='%s', delimiter=',', header=header, comments='')\n",
    "\n",
    "\n",
    "def saveCache(cache, verbose = common_core.verbose):\n",
    "    '''Save all extracted data to CSV'''\n",
    "\n",
    "    for nationName in common_core.nationNames:\n",
    "        saveArea(cache, \"nation\", nationName, verbose = verbose)\n",
    "        \n",
    "    for regionName in common_core.regionNames:\n",
    "        saveArea(cache, \"region\", regionName, verbose = verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download of weekly/publishedweek012021.xlsx...\n",
      "Skipping download of weekly/publishedweek532020.xlsx...\n",
      "Skipping download of weekly/publishedweek522019.xls...\n",
      "Skipping download of weekly/publishedweek522018withupdatedrespiratoryrow.xls...\n",
      "Skipping download of weekly/publishedweek522017.xls...\n",
      "Skipping download of weekly/publishedweek522016.xls...\n",
      "Skipping download of weekly/publishedweek2015.xls...\n",
      "Skipping download of weekly/publishedweek2014.xls...\n",
      "Skipping download of weekly/publishedweek2013.xls...\n",
      "Skipping download of weekly/publishedweek2012.xls...\n",
      "Skipping download of weekly/publishedweek2011.xls...\n",
      "Skipping download of weekly/publishedweek2010.xls...\n",
      "Initialising England and Wales...\n",
      "Initialising Wales...\n",
      "Initialising North East...\n",
      "Initialising North West...\n",
      "Initialising Yorkshire and The Humber...\n",
      "Initialising East Midlands...\n",
      "Initialising West Midlands...\n",
      "Initialising East of England...\n",
      "Initialising London...\n",
      "Initialising South East...\n",
      "Initialising South West...\n",
      "Initialising England...\n",
      "Saving England and Wales...\n",
      "Saving England...\n",
      "Saving Wales...\n",
      "Saving North East...\n",
      "Saving North West...\n",
      "Saving Yorkshire and The Humber...\n",
      "Saving East Midlands...\n",
      "Saving West Midlands...\n",
      "Saving East of England...\n",
      "Saving London...\n",
      "Saving South East...\n",
      "Saving South West...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    # Check / download latest spreadsheets\n",
    "    partNames = ons_download.downloadDeaths(verbose = verbose)\n",
    "    cache = loadExcelFiles(partNames, verbose = verbose)\n",
    "\n",
    "    # Tidy up the data, prior to saving as CSV files\n",
    "    createEngland(cache, verbose = verbose)\n",
    "    trimCache(cache, verbose = verbose)\n",
    "    polyfillCache(cache, verbose = verbose)\n",
    "\n",
    "    # Save cache as CSV files\n",
    "    saveCache(cache, verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
