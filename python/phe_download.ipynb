{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHE Download\n",
    "\n",
    "Created by Michael George (AKA Logiqx)\n",
    "\n",
    "Website: https://logiqx.github.io/covid-stats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "projdir = os.path.realpath(os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printable Class\n",
    "\n",
    "Simple class that allows other classes to be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Printable:\n",
    "    def __repr__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Data to download via the API - cases, patients, deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = \"https://api.coronavirus.data.gov.uk/v1/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "casesStructure = {\n",
    "    \"cases\": \"newCasesBySpecimenDate\", # Cases by specimen date\n",
    "    \"casesRollingSum\": \"newCasesBySpecimenDateRollingSum\", # Total cases (7-day average)\n",
    "    \"casesRollingRate\": \"newCasesBySpecimenDateRollingRate\", # Rate of cases per 100K over 7 days\n",
    "    \"casesAgeDemographics\": \"newCasesBySpecimenDateAgeDemographics\", # Demographics\n",
    "    \"casesReported\": \"newCasesByPublishDate\" # Cases by date reported\n",
    "}\n",
    "\n",
    "patientsStructure = {\n",
    "    \"hospitalAdmissions\": \"newAdmissions\", # Patients admitted to hospital\n",
    "    \"hospitalPatients\": \"hospitalCases\", # Patients in hospital\n",
    "    \"hospitalPatientsMv\": \"covidOccupiedMVBeds\" # Patients in mechanical ventilation beds\n",
    "}\n",
    "\n",
    "deathsStructure = {\n",
    "    \"deaths\": \"newDeaths28DaysByDeathDate\", # Deaths within 28d of +ve test by date of death\n",
    "    \"deathsRollingSum\": \"newDeaths28DaysByDeathDateRollingSum\", # Total deaths by date of death (7-day average)\n",
    "    \"deathsRollingRate\": \"newDeaths28DaysByDeathDateRollingRate\", # Rate of deaths per 100K over 7 days\n",
    "    \"deathsAgeDemographics\": \"newDeaths28DaysByDeathDateAgeDemographics\", # Demographics\n",
    "    \"deathsReported\": \"newDeaths28DaysByPublishDate\" # Deaths within 28d of +ve test by date reported\n",
    "}\n",
    "\n",
    "onsStructure = {\n",
    "    \"deathsRegistered\": \"newOnsDeathsByRegistrationDate\" # COVID-19 on the death certificate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageDemographics = [\n",
    "    '00_04',\n",
    "    '05_09',\n",
    "    '10_14',\n",
    "    '15_19',\n",
    "    '20_24',\n",
    "    '25_29',\n",
    "    '30_34',\n",
    "    '35_39',\n",
    "    '40_44',\n",
    "    '45_49',\n",
    "    '50_54',\n",
    "    '55_59',\n",
    "    '60_64',\n",
    "    '65_69',\n",
    "    '70_74',\n",
    "    '75_79',\n",
    "    '80_84',\n",
    "    '85_89',\n",
    "    '90+'    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Class\n",
    "\n",
    "Download data via the API and prepare it for analysis.\n",
    "\n",
    "Supports nations, regions and LTLAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class Area(Printable):\n",
    "    def __init__(self, areaType, areaName):\n",
    "        \"\"\"Initialisise the area object\"\"\"\n",
    "\n",
    "        self.areaType = areaType\n",
    "        self.areaName = areaName\n",
    "        self.safeName = areaName.lower().replace(' ', '_').replace(',', '')\n",
    "        for word in 'of', 'and', 'the':\n",
    "            self.safeName = self.safeName.replace(word + '_', '')\n",
    "        self.csvName = self.safeName + '.csv'\n",
    "        \n",
    "\n",
    "    def getRawPath(self, period):\n",
    "        \"\"\"Get path for raw data\"\"\"\n",
    "        rawPath = os.path.join(projdir, \"data\", \"phe-dashboard\", \"raw\", period, self.areaType)\n",
    "\n",
    "        return rawPath\n",
    "\n",
    "        \n",
    "    def getCsvPath(self, period, category):\n",
    "        \"\"\"Get path for csv data\"\"\"\n",
    "        csvPath = os.path.join(projdir, \"data\", \"phe-dashboard\", \"csv\", period, category, self.areaType)\n",
    "\n",
    "        return csvPath\n",
    "\n",
    "        \n",
    "    def download(self, period = \"daily\"):\n",
    "        \"\"\"Download data from PHE dashboard\"\"\"\n",
    "\n",
    "        # Catch all exceptions\n",
    "        try:\n",
    "            filters = [\n",
    "                f\"areaType={self.areaType}\",\n",
    "                f\"areaName={self.areaName}\"\n",
    "            ]\n",
    "\n",
    "            structure = {\n",
    "                \"date\": \"date\",\n",
    "                \"areaName\": \"areaName\"\n",
    "            }\n",
    "\n",
    "            if period == \"weekly\":\n",
    "                if self.areaType in ['nation', 'region', 'ltla']:\n",
    "                    structure.update(onsStructure)\n",
    "            else:\n",
    "                if self.areaType in ['nation', 'region', 'ltla']:\n",
    "                    structure.update(casesStructure)\n",
    "                    structure.update(deathsStructure)\n",
    "                if self.areaType in ['nation', 'nhsregion']:\n",
    "                    structure.update(patientsStructure)\n",
    "\n",
    "            api_params = {\n",
    "                \"filters\": str.join(\";\", filters),\n",
    "                \"structure\": json.dumps(structure, separators=(\",\", \":\")),\n",
    "                \"format\": \"csv\"\n",
    "            }\n",
    "\n",
    "            # Download raw data - hybrid of CSV and Python dictionaries\n",
    "            response = requests.get(ENDPOINT, params=api_params, timeout=10)\n",
    "            assert response.status_code == 200, f\"Failed request for {self.areaName}: {response.status_code} {response.text}\"\n",
    "\n",
    "            # Ensure raw path exists\n",
    "            rawPath = self.getRawPath(period)\n",
    "            if not os.path.exists(rawPath):\n",
    "                os.makedirs(rawPath)\n",
    "\n",
    "            # Save raw data\n",
    "            rawFn = os.path.join(rawPath, self.csvName)\n",
    "            with open(rawFn, 'w') as f:\n",
    "                f.write(response.content.decode())\n",
    "\n",
    "        # General catch all to report exceptions then abort\n",
    "        except:\n",
    "            print(f\"Failed to download {period} data for {self.areaName}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def downloadDaily(self):\n",
    "        \"\"\"Download daily data for analysis\"\"\"\n",
    "\n",
    "        print(f\"Downloading {self.areaName}...\")\n",
    "        self.download()\n",
    "\n",
    "\n",
    "    def downloadWeekly(self):\n",
    "        \"\"\"Download weekly data for analysis\"\"\"\n",
    "\n",
    "        if self.areaType in ['nation', 'region', 'ltla']:\n",
    "            print(f\"Downloading {self.areaName}...\")\n",
    "            self.download(\"weekly\")\n",
    "\n",
    "\n",
    "    def prepare(self, category, period = \"daily\"):\n",
    "        \"\"\"Prepare data for analysis\"\"\"\n",
    "\n",
    "        def getColNos(row, category):\n",
    "            \"\"\"Get column numbers relevant to the category\"\"\"\n",
    "\n",
    "            if category == \"cases\":\n",
    "                structure = casesStructure\n",
    "            elif category == \"patients\":\n",
    "                structure = patientsStructure\n",
    "            elif category == \"deaths\":\n",
    "                structure = deathsStructure\n",
    "            elif category == \"ons\":\n",
    "                structure = onsStructure\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported category - {category}\") \n",
    "\n",
    "            # Always include date and areaName\n",
    "            colNos = [0, 1]\n",
    "\n",
    "            # Other columns are dependent on the category and its structure\n",
    "            for colNo in range(len(row)):\n",
    "                if row[colNo] in structure:\n",
    "                    if row[colNo].endswith(\"Demographics\"):\n",
    "                        colNos.append(colNo)\n",
    "                    else:\n",
    "                        colNos.append(colNo)\n",
    "                \n",
    "            return colNos\n",
    "\n",
    "\n",
    "        def getColNames(row, colNos):\n",
    "            \"\"\"Get column names\"\"\"\n",
    "\n",
    "            # Always include date and areaName\n",
    "            colNames = []\n",
    "\n",
    "            # Other columns are dependent on the category and its structure\n",
    "            for colNo in colNos:\n",
    "                if row[colNo].endswith(\"Demographics\"):\n",
    "                    for ageDemographic in ageDemographics:\n",
    "                        colNames.append(f\"{category}{ageDemographic}\")\n",
    "                        colNames.append(f\"{category}RollingSum{ageDemographic}\")\n",
    "                        colNames.append(f\"{category}RollingRate{ageDemographic}\")\n",
    "                else:\n",
    "                    colNames.append(row[colNo])\n",
    "                \n",
    "            return colNames\n",
    "\n",
    "\n",
    "        def getRowValues(row, colNos):\n",
    "            \"\"\"Get row values\"\"\"\n",
    "\n",
    "            tidyRow = []\n",
    "            for colNo in colNos:\n",
    "                if row[colNo].startswith('['):\n",
    "                    items = eval(row[colNo])\n",
    "                    for ageDemographic in ageDemographics:\n",
    "                        found = False\n",
    "                        for item in items:\n",
    "                            if item[\"age\"] == ageDemographic:\n",
    "                                tidyRow.append(item[category])\n",
    "                                tidyRow.append(item[\"rollingSum\"])\n",
    "                                tidyRow.append(item[\"rollingRate\"])\n",
    "                                found = True\n",
    "                                break\n",
    "\n",
    "                        if found == False:\n",
    "                            tidyRow.append(\"\")\n",
    "                            tidyRow.append(\"\")\n",
    "                            tidyRow.append(\"\")\n",
    "                else:\n",
    "                    tidyRow.append(row[colNo])\n",
    "\n",
    "            return tidyRow\n",
    "\n",
    "\n",
    "        # Catch all exceptions\n",
    "        try:\n",
    "            # Determine raw filename\n",
    "            rawPath = self.getRawPath(period)\n",
    "            rawFn = os.path.join(rawPath, self.csvName)\n",
    "\n",
    "            # Ensure the CSV path exists\n",
    "            csvPath = self.getCsvPath(period, category)\n",
    "            if not os.path.exists(csvPath):\n",
    "                os.makedirs(csvPath)\n",
    "\n",
    "            # Generate the CSV from raw data\n",
    "            csvFn = os.path.join(csvPath, self.csvName)\n",
    "            with open(csvFn, 'w') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                with open(rawFn, 'r') as f:\n",
    "                    reader = csv.reader(f, delimiter = ',')\n",
    "                    rows = []\n",
    "                    rowNo = 0\n",
    "                    for row in reader:\n",
    "                        if rowNo == 0:\n",
    "                            colNos = getColNos(row, category)\n",
    "                            colNames = getColNames(row, colNos)\n",
    "                        else:\n",
    "                            row = getRowValues(row, colNos)\n",
    "                            rows.append(row)\n",
    "\n",
    "                        rowNo += 1\n",
    "\n",
    "                    rows.reverse()\n",
    "                    writer.writerow(colNames)\n",
    "                    writer.writerows(rows)\n",
    "\n",
    "        # General catch all to report exceptions then abort\n",
    "        except:\n",
    "            print(f\"Failed to convert {period} {category} for {self.areaName}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def prepareDaily(self):\n",
    "        \"\"\"Prepare daily data for analysis\"\"\"\n",
    "\n",
    "        print(f\"Preparing {self.areaName}...\")\n",
    "\n",
    "        if self.areaType in [\"nation\", \"region\", \"ltla\"]:\n",
    "            self.prepare(\"cases\")\n",
    "            self.prepare(\"deaths\")\n",
    "        if self.areaType in [\"nation\", \"nhsregion\"]:\n",
    "            self.prepare(\"patients\")\n",
    "\n",
    "\n",
    "    def prepareWeekly(self):\n",
    "        \"\"\"Prepare weekly data for analysis\"\"\"\n",
    "\n",
    "        if self.areaType in [\"nation\", \"region\", \"ltla\"]:\n",
    "            print(f\"Preparing {self.areaName}...\")\n",
    "            self.prepare(\"ons\", \"weekly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationNames = [\"England\"]\n",
    "\n",
    "regionNames = [\"North West\", \"North East\", \"Yorkshire and The Humber\", \"West Midlands\", \"East Midlands\",\n",
    "             \"East of England\", \"London\", \"South East\", \"South West\"]\n",
    "\n",
    "nhsRegionNames = [\"North West\", \"North East and Yorkshire\", \"Midlands\",\n",
    "                  \"East of England\", \"London\", \"South East\", \"South West\"]\n",
    "\n",
    "ltlaNames = [\"Dorset\", \"Bournemouth, Christchurch and Poole\",\n",
    "             \"Stevenage\", \"Welwyn Hatfield\", \"North Hertfordshire\", \"East Hertfordshire\",\n",
    "             \"Sandwell\", \"Dudley\", \"Birmingham\",\n",
    "             \"Derbyshire Dales\", \"North East Derbyshire\", \"High Peak\", \"Sheffield\",\n",
    "             \"Croydon\"]\n",
    "\n",
    "areas = [(\"nation\", [\"England\"]), (\"region\", regionNames), (\"nhsregion\", nhsRegionNames), (\"ltla\", ltlaNames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Daily Data ---\n",
      "Downloading England...\n",
      "Downloading North West...\n",
      "Downloading North East...\n",
      "Downloading Yorkshire and The Humber...\n",
      "Downloading West Midlands...\n",
      "Downloading East Midlands...\n",
      "Downloading East of England...\n",
      "Downloading London...\n",
      "Downloading South East...\n",
      "Downloading South West...\n",
      "Downloading North West...\n",
      "Downloading North East and Yorkshire...\n",
      "Downloading Midlands...\n",
      "Downloading East of England...\n",
      "Downloading London...\n",
      "Downloading South East...\n",
      "Downloading South West...\n",
      "Downloading Dorset...\n",
      "Downloading Bournemouth, Christchurch and Poole...\n",
      "Downloading Stevenage...\n",
      "Downloading Welwyn Hatfield...\n",
      "Downloading North Hertfordshire...\n",
      "Downloading East Hertfordshire...\n",
      "Downloading Sandwell...\n",
      "Downloading Dudley...\n",
      "Downloading Birmingham...\n",
      "Downloading Derbyshire Dales...\n",
      "Downloading North East Derbyshire...\n",
      "Downloading High Peak...\n",
      "Downloading Sheffield...\n",
      "Downloading Croydon...\n",
      "\n",
      "--- Weekly Data ---\n",
      "Downloading England...\n",
      "Downloading North West...\n",
      "Downloading North East...\n",
      "Downloading Yorkshire and The Humber...\n",
      "Downloading West Midlands...\n",
      "Downloading East Midlands...\n",
      "Downloading East of England...\n",
      "Downloading London...\n",
      "Downloading South East...\n",
      "Downloading South West...\n",
      "Downloading Dorset...\n",
      "Downloading Bournemouth, Christchurch and Poole...\n",
      "Downloading Stevenage...\n",
      "Downloading Welwyn Hatfield...\n",
      "Downloading North Hertfordshire...\n",
      "Downloading East Hertfordshire...\n",
      "Downloading Sandwell...\n",
      "Downloading Dudley...\n",
      "Downloading Birmingham...\n",
      "Downloading Derbyshire Dales...\n",
      "Downloading North East Derbyshire...\n",
      "Downloading High Peak...\n",
      "Downloading Sheffield...\n",
      "Downloading Croydon...\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Daily Data ---\")\n",
    "\n",
    "for areaType, areaNames in areas:\n",
    "    for areaName in areaNames:\n",
    "        area = Area(areaType, areaName)\n",
    "        area.downloadDaily()\n",
    "        \n",
    "print(\"\\n--- Weekly Data ---\")\n",
    "\n",
    "for areaType, areaNames in areas:\n",
    "    for areaName in areaNames:\n",
    "        area = Area(areaType, areaName)\n",
    "        area.downloadWeekly()\n",
    "        \n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing daily data ---\n",
      "Preparing England...\n",
      "Preparing North West...\n",
      "Preparing North East...\n",
      "Preparing Yorkshire and The Humber...\n",
      "Preparing West Midlands...\n",
      "Preparing East Midlands...\n",
      "Preparing East of England...\n",
      "Preparing London...\n",
      "Preparing South East...\n",
      "Preparing South West...\n",
      "Preparing North West...\n",
      "Preparing North East and Yorkshire...\n",
      "Preparing Midlands...\n",
      "Preparing East of England...\n",
      "Preparing London...\n",
      "Preparing South East...\n",
      "Preparing South West...\n",
      "Preparing Dorset...\n",
      "Preparing Bournemouth, Christchurch and Poole...\n",
      "Preparing Stevenage...\n",
      "Preparing Welwyn Hatfield...\n",
      "Preparing North Hertfordshire...\n",
      "Preparing East Hertfordshire...\n",
      "Preparing Sandwell...\n",
      "Preparing Dudley...\n",
      "Preparing Birmingham...\n",
      "Preparing Derbyshire Dales...\n",
      "Preparing North East Derbyshire...\n",
      "Preparing High Peak...\n",
      "Preparing Sheffield...\n",
      "Preparing Croydon...\n",
      "\n",
      "---Preparing weekly data ---\n",
      "Preparing England...\n",
      "Preparing North West...\n",
      "Preparing North East...\n",
      "Preparing Yorkshire and The Humber...\n",
      "Preparing West Midlands...\n",
      "Preparing East Midlands...\n",
      "Preparing East of England...\n",
      "Preparing London...\n",
      "Preparing South East...\n",
      "Preparing South West...\n",
      "Preparing Dorset...\n",
      "Preparing Bournemouth, Christchurch and Poole...\n",
      "Preparing Stevenage...\n",
      "Preparing Welwyn Hatfield...\n",
      "Preparing North Hertfordshire...\n",
      "Preparing East Hertfordshire...\n",
      "Preparing Sandwell...\n",
      "Preparing Dudley...\n",
      "Preparing Birmingham...\n",
      "Preparing Derbyshire Dales...\n",
      "Preparing North East Derbyshire...\n",
      "Preparing High Peak...\n",
      "Preparing Sheffield...\n",
      "Preparing Croydon...\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Preparing daily data ---\")\n",
    "\n",
    "for areaType, areaNames in areas:\n",
    "    for areaName in areaNames:\n",
    "        area = Area(areaType, areaName)\n",
    "        area.prepareDaily()\n",
    "\n",
    "print(\"\\n---Preparing weekly data ---\")\n",
    "\n",
    "for areaType, areaNames in areas:\n",
    "    for areaName in areaNames:\n",
    "        area = Area(areaType, areaName)\n",
    "        area.prepareWeekly()\n",
    "        \n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
